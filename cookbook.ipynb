{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Cookbook: Fundamentals of Web scraping & Data wrangling</h1>\n",
    "<p>\n",
    "Web scraping is an important part of data collection. A lot of the time, useful data won't be readily available to download. With web scraping, we can automate the process of gathering data from multiple web pages, saving us time and effort.\n",
    "\n",
    "By scraping websites, we can access a wide range of data, including text, images, tables, and more. This data can be used for various purposes, such as market research, sentiment analysis, price comparison, content aggregation, and data analysis.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic website is set up for demo.<br>\n",
    "run `python app.py` to start server before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Basic web requests</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>The libraries **requests**, **BeautifulSoup** and **lxml** are commonly used in Python for web scraping and web content retrieval tasks. \n",
    "\n",
    "requests: It is a powerful library that allows you to send HTTP requests to web pages and web services. With requests, you can easily retrieve HTML content, make GET and POST requests, handle cookies and sessions, and interact with web APIs. It's an essential tool for fetching web data and interacting with web resources programmatically.\n",
    "\n",
    "BeautifulSoup: This library is a popular choice for parsing and navigating HTML and XML documents. It provides a convenient way to extract specific data from web pages by traversing the HTML document's structure. You can search for tags, access tag attributes, and extract text or data of interest. When used in combination with requests, BeautifulSoup becomes a powerful tool for web scraping and data extraction.\n",
    "\n",
    "lxml is a sub library of pandas (so when importing it, just importing pandas is enough, assuming you have it installed) that is used to parse HTML efficiently.\n",
    "\n",
    "Together, these libraries enable you to access web content, retrieve information, and perform data extraction tasks efficiently.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the dependencies, so if you don't have them, you'll need to install them with pip or whatever package manager you use\n",
    "# pip install requests, bs4, pandas, lxml\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most basic usage of request. requests.get() send a GET request to the designated url, and returns the entire site's content as response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = \"http://127.0.0.1:5000/\" # URL of the website\n",
    "response = requests.get(url) # get the response from the website as a response object\n",
    "print(response) # print the response object, though it's not what you'd expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a response object isn't useful to us just yet. It contains more information than just the site's source code. Depending on the site content, it could be text, json, image, api request, etc. Here we will just read the source code as text to see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Document</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h3>click link to see tables</h3>\n",
      "    <a href=\"/table1\">table 1</a>\n",
      "    <a href=\"/table2\">table 2</a>\n",
      "    <a href=\"/\">home</a>\n",
      "    <h3>API guide</h3>\n",
      "    <p>access subdomain 'table-data' and 'table2-data' to retrieve data directly</p>\n",
      "    <p>stock image</p>\n",
      "    <img alt=\"stock\" src=\"/static/stock.png\"></img>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "response_text = response.text # turn the response object into a string\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling image is a bit different, since you can't print it out directly. In order to keep the image, you must save it to a file.\n",
    "We see an image present on the site: `<img alt=\"stock\" src=\"/static/stock.png\"></img>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the image URL\n",
    "response = requests.get(url + '/static/stock.png')\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the image locally\n",
    "    with open(\"saved_image.png\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "        print(\"Image saved successfully.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the image.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Parsing html for data</h2>\n",
    "In order to parse the information we extracted from the site, and extract the table we want, we use the functions of BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      "<tr>\n",
      "<th>Name</th>\n",
      "<th>Age</th>\n",
      "<th>City</th>\n",
      "<th>Occupation</th>\n",
      "<th>Salary</th>\n",
      "<th>Hire Date</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>John Doe</td>\n",
      "<td>25</td>\n",
      "<td>New York</td>\n",
      "<td>Software Engineer</td>\n",
      "<td>$100,000</td>\n",
      "<td>2022-09-01</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Jane Smith</td>\n",
      "<td>30</td>\n",
      "<td>Los Angeles</td>\n",
      "<td>Graphic Designer</td>\n",
      "<td>$80,000</td>\n",
      "<td>2022-09-02</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Mike Johnson</td>\n",
      "<td>35</td>\n",
      "<td>Chicago</td>\n",
      "<td>Marketing Manager</td>\n",
      "<td>$120,000</td>\n",
      "<td>2022-09-03</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Sarah Williams</td>\n",
      "<td>28</td>\n",
      "<td>San Francisco</td>\n",
      "<td>Data Analyst</td>\n",
      "<td>$90,000</td>\n",
      "<td>2022-09-04</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>David Brown</td>\n",
      "<td>32</td>\n",
      "<td>Miami</td>\n",
      "<td>Product Manager</td>\n",
      "<td>$110,000</td>\n",
      "<td>2022-09-05</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Emily Davis</td>\n",
      "<td>27</td>\n",
      "<td>Boston</td>\n",
      "<td>UX Designer</td>\n",
      "<td>$95,000</td>\n",
      "<td>2022-09-06</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Michael Johnson</td>\n",
      "<td>33</td>\n",
      "<td>Chicago</td>\n",
      "<td>Marketing Manager</td>\n",
      "<td>$120,000</td>\n",
      "<td>2022-09-03</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Anna Thompson</td>\n",
      "<td>29</td>\n",
      "<td>San Francisco</td>\n",
      "<td>Data Analyst</td>\n",
      "<td>$90,000</td>\n",
      "<td>2022-09-04</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>David Brown</td>\n",
      "<td>32</td>\n",
      "<td>Miami</td>\n",
      "<td>Product Manager</td>\n",
      "<td>$110,000</td>\n",
      "<td>2022-09-05</td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "table1 = requests.get(url + 'table1') # get the table as a repsonse object\n",
    "soup = bs(table1.text, 'html.parser') # convert response object into text, then into a soup object throught html.parser\n",
    "table1_processed = soup.find('table') # find any tables in the soup object\n",
    "print(table1_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing it and keeping only the table, we use the functions of lxml to parse it again and turn it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Name  Age           City         Occupation    Salary   Hire Date\n",
      "0        John Doe   25       New York  Software Engineer  $100,000  2022-09-01\n",
      "1      Jane Smith   30    Los Angeles   Graphic Designer   $80,000  2022-09-02\n",
      "2    Mike Johnson   35        Chicago  Marketing Manager  $120,000  2022-09-03\n",
      "3  Sarah Williams   28  San Francisco       Data Analyst   $90,000  2022-09-04\n",
      "4     David Brown   32          Miami    Product Manager  $110,000  2022-09-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_22288\\158322927.py:1: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_table1 = pd.read_html(str(table1_processed))[0]\n"
     ]
    }
   ],
   "source": [
    "df_table1 = pd.read_html(str(table1_processed))[0]\n",
    "print(df_table1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data wrangling</h2>\n",
    "If you look at the site, you'll see there are two tables. There are no benefits for the two tables to be seperate, but for some reason they are. So in order to make using the data easier, we need to combine the tables into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Name  Age           City         Occupation    Salary   Hire Date\n",
      "0        John Doe   25       New York  Software Engineer  $100,000  2022-09-01\n",
      "1      Jane Smith   30    Los Angeles   Graphic Designer   $80,000  2022-09-02\n",
      "2    Mike Johnson   35        Chicago  Marketing Manager  $120,000  2022-09-03\n",
      "3  Sarah Williams   28  San Francisco       Data Analyst   $90,000  2022-09-04\n",
      "4     David Brown   32          Miami    Product Manager  $110,000  2022-09-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_22288\\3038512959.py:5: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_table2 = pd.read_html(str(table1_processed))[0]\n"
     ]
    }
   ],
   "source": [
    "# Getting the second table\n",
    "table2 = requests.get(url + 'table1')\n",
    "soup2 = bs(table1.text, 'html.parser')\n",
    "table2_processed = soup2.find('table')\n",
    "df_table2 = pd.read_html(str(table1_processed))[0]\n",
    "print(df_table1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name  Age_x         City_x       Occupation_x  Salary_x  \\\n",
      "0          John Doe     25       New York  Software Engineer  $100,000   \n",
      "1        Jane Smith     30    Los Angeles   Graphic Designer   $80,000   \n",
      "2      Mike Johnson     35        Chicago  Marketing Manager  $120,000   \n",
      "3    Sarah Williams     28  San Francisco       Data Analyst   $90,000   \n",
      "4       David Brown     32          Miami    Product Manager  $110,000   \n",
      "5       David Brown     32          Miami    Product Manager  $110,000   \n",
      "6       David Brown     32          Miami    Product Manager  $110,000   \n",
      "7       David Brown     32          Miami    Product Manager  $110,000   \n",
      "8       Emily Davis     27         Boston        UX Designer   $95,000   \n",
      "9   Michael Johnson     33        Chicago  Marketing Manager  $120,000   \n",
      "10    Anna Thompson     29  San Francisco       Data Analyst   $90,000   \n",
      "\n",
      "   Hire Date_x  Age_y         City_y       Occupation_y  Salary_y Hire Date_y  \n",
      "0   2022-09-01     25       New York  Software Engineer  $100,000  2022-09-01  \n",
      "1   2022-09-02     30    Los Angeles   Graphic Designer   $80,000  2022-09-02  \n",
      "2   2022-09-03     35        Chicago  Marketing Manager  $120,000  2022-09-03  \n",
      "3   2022-09-04     28  San Francisco       Data Analyst   $90,000  2022-09-04  \n",
      "4   2022-09-05     32          Miami    Product Manager  $110,000  2022-09-05  \n",
      "5   2022-09-05     32          Miami    Product Manager  $110,000  2022-09-05  \n",
      "6   2022-09-05     32          Miami    Product Manager  $110,000  2022-09-05  \n",
      "7   2022-09-05     32          Miami    Product Manager  $110,000  2022-09-05  \n",
      "8   2022-09-06     27         Boston        UX Designer   $95,000  2022-09-06  \n",
      "9   2022-09-03     33        Chicago  Marketing Manager  $120,000  2022-09-03  \n",
      "10  2022-09-04     29  San Francisco       Data Analyst   $90,000  2022-09-04  \n"
     ]
    }
   ],
   "source": [
    "# Merging the two tables using built-in pandas function\n",
    "combined_df = df_table1.merge(df_table2, on='Name')\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 'Name' column is the only column that both tables have in common, so we use that as the key\n",
    "If you want to merge on multiple columns, you can do something like this:\n",
    "combined_df = df_table1.merge(df_table2, on=['Name', 'Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using APIs</h2>\n",
    "While some sites doesn't provide ways to retrieve data, requiring you to scrape data off the site directly, some do offer built in methods of fetching data directly(and various other automation functions) in the form of API. APIs are used in a wide range of applications, from web and mobile app development to cloud services integration, IoT (Internet of Things), and more. They enable developers to leverage existing services and functionality, reducing development time and effort while promoting interoperability and flexibility in software ecosystems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provide sample site has a simplistic API built in, which allows users to retrieve the data tables in the form of JSON. If you nagivate directly to the links, you'll see a raw JSON table. This is much easier to process compared to extracting the tables from html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Email</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Department</th>\n",
       "      <th>Manager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>123-456-7890</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Jane Wilson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>jane.smith@example.com</td>\n",
       "      <td>234-567-8901</td>\n",
       "      <td>Design</td>\n",
       "      <td>Robert Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike Johnson</td>\n",
       "      <td>mike.johnson@example.com</td>\n",
       "      <td>345-678-9012</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Emily Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah Williams</td>\n",
       "      <td>sarah.williams@example.com</td>\n",
       "      <td>456-789-0123</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Michael Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David Brown</td>\n",
       "      <td>david.brown@example.com</td>\n",
       "      <td>567-890-1234</td>\n",
       "      <td>Product</td>\n",
       "      <td>Lisa White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name                       Email  Phone Number    Department  \\\n",
       "0        John Doe        john.doe@example.com  123-456-7890   Engineering   \n",
       "1      Jane Smith      jane.smith@example.com  234-567-8901        Design   \n",
       "2    Mike Johnson    mike.johnson@example.com  345-678-9012     Marketing   \n",
       "3  Sarah Williams  sarah.williams@example.com  456-789-0123  Data Science   \n",
       "4     David Brown     david.brown@example.com  567-890-1234       Product   \n",
       "\n",
       "         Manager  \n",
       "0    Jane Wilson  \n",
       "1   Robert Black  \n",
       "2    Emily Green  \n",
       "3  Michael Brown  \n",
       "4     Lisa White  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URLs for the APIs\n",
    "url_table_data = url+'table-data'\n",
    "url_table2_data = url+'table2-data'\n",
    "\n",
    "# Making GET requests to the APIs\n",
    "response_table_data = requests.get(url_table_data)\n",
    "response_table2_data = requests.get(url_table2_data)\n",
    "\n",
    "# Assuming the APIs return JSON data, parse the JSON into Python dictionaries\n",
    "table_data = response_table_data.json()\n",
    "table2_data = response_table2_data.json()\n",
    "# Turn the JSON output into a Pandas DataFrame for further analysis\n",
    "table_dp = pd.DataFrame(table_data)\n",
    "table2_dp = pd.DataFrame(table2_data)\n",
    "table_dp.head()\n",
    "table2_dp.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
